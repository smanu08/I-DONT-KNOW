{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo38FXDFjHhdu9x2zGdR7X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smanu08/I-DONT-KNOW/blob/main/summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4EfsLfEX4z2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e571c39c-1bd1-4463-b91c-efea8e892498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.71+cuda111)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.7.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.5.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.25)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->trax) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.11.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.4.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.17.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.4.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (21.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->trax) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.22.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.42.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (12.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.13.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (4.8.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow-text->trax) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=70)\n",
        "!pip install trax\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as jnp\n",
        "\n",
        "# to print the entire np array\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This will download the dataset if no data_dir is specified.\n",
        "# Downloading and processing can take bit of time,\n",
        "# so we have the data already in 'data/' for you\n",
        "\n",
        "# Importing CNN/DailyMail articles dataset\n",
        "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
        "                                 data_dir='data/',\n",
        "                                 keys=('article', 'highlights'),\n",
        "                                 train=True)\n",
        "\n",
        "# This should be much faster as the data is downloaded already.\n",
        "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
        "                                data_dir='data/',\n",
        "                                keys=('article', 'highlights'),\n",
        "                                train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzmdvIYv5Ctt",
        "outputId": "7de26562-e0d8-4981-e2f7-fcd64f58278c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:413: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(input_str, EOS=1):\n",
        "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "  \n",
        "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
        "    # we get around it by making a 1-element stream with `iter`.\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      \n",
        "                                      vocab_file='en_8k.subword'))\n",
        "    \n",
        "    # Mark the end of the sentence with EOS\n",
        "    return list(inputs) + [EOS]\n",
        "\n",
        "def detokenize(integers):\n",
        "    \"\"\"List of ints to str\"\"\"\n",
        "  \n",
        "    s = trax.data.detokenize(integers,\n",
        "                             \n",
        "                             vocab_file='en_8k.subword')\n",
        "    \n",
        "    return wrapper.fill(s)"
      ],
      "metadata": {
        "id": "F2xq_d7i7nSF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Special tokens\n",
        "SEP = 0 # Padding or separator token\n",
        "EOS = 1 # End of sentence token\n",
        "\n",
        "# Concatenate tokenized inputs and targets using 0 as separator.\n",
        "def preprocess(stream):\n",
        "    for (article, summary) in stream:\n",
        "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
        "        yield joint, joint, np.array(mask)\n",
        "\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\n",
        "input_pipeline = trax.data.Serial(\n",
        "    # Tokenizes\n",
        "    trax.data.Tokenize(\n",
        "                       vocab_file='en_8k.subword'),\n",
        "    # Uses function defined above\n",
        "    preprocess,\n",
        "    # Filters out examples longer than 2048\n",
        "    trax.data.FilterByLength(2048)\n",
        ")\n",
        "\n",
        "# Apply preprocessing to data streams.\n",
        "train_stream = input_pipeline(train_stream_fn())\n",
        "eval_stream = input_pipeline(eval_stream_fn())\n",
        "\n",
        "train_input, train_target, train_mask = next(train_stream)\n",
        "\n",
        "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM).\n"
      ],
      "metadata": {
        "id": "5KrPBzikMMsK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bbexdr7IMcRJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python trax/data/text_encoder_build_subword.py \\--corpus_filepattern=data/data.txt --corpus_max_lines=40000 \\--output_filename=data/my_file.subword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo0HSDLFKDCQ",
        "outputId": "2a357afa-b0d9-4a84-c13a-6e8087e09cab"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'trax/data/text_encoder_build_subword.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Special tokens\n",
        "SEP = 0 # Padding or separator token\n",
        "EOS = 1 # End of sentence token\n",
        "\n",
        "# Concatenate tokenized inputs and targets using 0 as separator.\n",
        "def preprocess(stream):\n",
        "    for (article, highlights) in stream:\n",
        "        joint = np.array(list(article) + [EOS, SEP] + list(highlights) + [EOS])\n",
        "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(highlights)) + 1) # Accounting for EOS and SEP\n",
        "        yield joint, joint, np.array(mask)\n",
        "\n",
        "# You can combine a few data preprocessing steps into a pipeline like this.\n",
        "input_pipeline = trax.data.Serial(\n",
        "    # Tokenizes\n",
        "    trax.data.Tokenize(\n",
        "                       vocab_file='summarize32k.subword.subwords'),\n",
        "    # Uses function defined above \n",
        "    preprocess,\n",
        "    # Filters out examples longer than 2048\n",
        "    trax.data.FilterByLength(2048)\n",
        ")\n",
        "\n",
        "# Apply preprocessing to data streams.\n",
        "train_stream = input_pipeline(train_stream_fn())\n",
        "eval_stream = input_pipeline(eval_stream_fn())\n",
        "\n",
        "train_input, train_target, train_mask = next(train_stream)\n",
        "\n",
        "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
      ],
      "metadata": {
        "id": "hie0lNnf78rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hDw2li64Flz_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prints mask, 0s on article, 1s on summary\n",
        "print(f'Single example mask:\\n\\n {train_mask}')"
      ],
      "metadata": {
        "id": "vd1SFXif9qcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4913e5-3663-4a5a-f5ca-03ec70f9a26b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single example mask:\n",
            "\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
        "print(f'Single example:\\n\\n {detokenize(train_input)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eByJZDJ8Kqza",
        "outputId": "36ac8011-026d-450b-ced9-8a0c8ae36504"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single example:\n",
            "\n",
            " QPR chairman Tony Fernandes has insisted his club can afford not to\n",
            "win promotion to the Premier League, despite debts of £177.1 million.\n",
            "Rangers face Derby County in the Championship play-off final at\n",
            "Wembley on May 24, with Harry Redknapp's side hoping to secure the\n",
            "£120m pay packet of Premier League promotion. But, should QPR return\n",
            "to the top tier at the first attempt, they could be forced to pay out\n",
            "more than half of that in fines under the Football League's Financial\n",
            "Fair Play regulations. We're ready: Queens Park Rangers chairman Tony\n",
            "Fernandes says his club doesn't have to win promotion . Off to\n",
            "Wembley: Rangers won their way through to the play-off final after\n",
            "extra-time against Wigan . Based on last year's accounts, Rangers\n",
            "would have to pay £62.1m if they are promoted because their £65.4m\n",
            "losses were so far in excess of the £8m allowed by the Football\n",
            "League. Should Redknapp's side stay in the Championship, however, they\n",
            "would be subjected to a transfer embargo. QPR have tried to reduce\n",
            "their wage bill by selling high-earners such as Christopher Samba and\n",
            "sending the likes of Loic Remy and Adel Taarabt out on loan. Improved:\n",
            "Businessman Fernandes says QPR are in a better financial position than\n",
            "two years ago . Winner: QPR striker Charlie Austin finds the net to\n",
            "snatch the victory in the play-off second leg . Fernandes told\n",
            "talkSPORT: 'Yes, we can (afford not to go up). I'm an accountant by\n",
            "background - although I may not seem it! 'We've told the fans, whether\n",
            "we go up or we don't, we're here for the long term. 'We know what\n",
            "culture we want at the club and we will continue the journey whether\n",
            "we're in the Championship or the Premier League. 'We are a much\n",
            "smarter, much wiser group of people than we were two years ago.'\n",
            "Relief: QPR's bid to reduce their wage bills included off-loading\n",
            "players such  as Christopher Samba . Major break: Loic Remy (right)\n",
            "went on loan to Newcastle and scored 14 goals for Alan Pardew's side\n",
            ".<EOS><pad>QueensPark Rangers have debts of £177.1 million . The club\n",
            "is also set to be hit with fines under Financial Fair Play rules . But\n",
            "chairman Tony Fernandes says they can survive without winning\n",
            "promotion to the Premier League this season . QPR take on Derby in the\n",
            "play-off final at Wembley on May 24 .<EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucketing to create batched generators.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n",
        "# 4 of length < 512. And so on. \n",
        "boundaries =  [128, 256,  512, 1024]\n",
        "batch_sizes = [16,    8,    4,    2, 1]\n",
        "\n",
        "# Create the streams.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes)(train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes)(eval_stream)"
      ],
      "metadata": {
        "id": "SC29-1RRMfFO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every execution will result in generation of a different article\n",
        "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
        "input_batch, _, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# Shape of the input_batch\n",
        "input_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k117EsKMmhy",
        "outputId": "71387972-1e84-44f7-9a02-8e0fcda9435a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1231)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print corresponding integer values\n",
        "print(input_batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4bS0iRYM2qR",
        "outputId": "6b30b7c6-aa56-49e3-d604-16e651252a5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 741  619  227    5 1000 5845 2174 2260  263    7  721 4932  527    3\n",
            " 3032  568    3  480 1177  963   42    7  547 1758 2315 4687   20    4\n",
            " 5267   52    5  131 4142 1080   23 1100 4558  220 1049 6140  602  288\n",
            "   58  124 3146 5916   74  265 1171   10 2034 2919   75    8   28  377\n",
            " 2675 5000 3370   23 3945   10    4  288   58    4 1084   20 6153   49\n",
            " 1804 6045  135    3  241  644 1376   44 5488 1773    5 2410 1340    3\n",
            " 3647    6 1125    8   28 4558 4244    6  103 3664   47  124 5699 4203\n",
            "    6   10   12 2270 1192 5082    6 1925   75   10 4711 2518  747  407\n",
            "  795    3 2953 2843 1492    2 1315    8   28  913 6045  135 6106   23\n",
            " 4142 1080   22    4 4558 4244    6   58    4  457  678  138    5    4\n",
            " 2843 1492 3198  569    7  119  696  782 3271  870 4876 3338   19 4402\n",
            " 2386    2  110 1125 4257    2 5308  798    2    3 3004  314 1109    5\n",
            "    4  913 6045  135   10 1024 1727 4579 1812    2    8   28 4558 4244\n",
            "    6    3   58 3075   13  959   83  673 1707    6    3  474  798   97\n",
            "  241 1419    5  725  162 1185 1202 8180    2 1343 1342 6072   67 1061\n",
            " 6680    5  335 3779  688 5308  798    2 1125 1202 8180    2  755 5004\n",
            "    2  185 6104 5328 5441  961  688 1343   42   14 4278 4876 3150  470\n",
            " 6424    4 2270 1192 5082    6   35  227   30   12  379   13 4891    2\n",
            " 5726    3 5308  798    2 1125    8   28 4558  220 1257   80 3650   20\n",
            "   10   12   95   14 2251    2   14   34   14   99   14 2251    2  531\n",
            " 1690  147    7 6363   57 1043  233   10   12 1925   75   58    4 2270\n",
            " 1192 5082    6 1202 8180    2  860  389  445 2185    6 5300   42   36\n",
            "   47   96   12  701 2712 3779  688 5308  798    2 1125 1202 8180    2\n",
            "  179   31  157   46  626   37  162   80 1258 2046   20   35   19 4456\n",
            "   73  479  227  961  688 1049 6140  602  288 2872 2501  920   42    3\n",
            "  162   80   10   12   48  122 3237   14 5292    2   14 2251    2 2576\n",
            " 5811   29  572   58    4  913 6045  135 1202 8180    2  755   55   10\n",
            "  124 1545 3274  369    3    7  162   55 3599   32  933  366 1669 3257\n",
            "   29 3779  688 2282   13 5308  798    2 1202 8180    2  755   55  349\n",
            " 5491   32    4 1697  961  688  191 5384    6   37    4 4558 4244    6\n",
            "  432 3151 2749    2   12 1496  115    5 1100 4558  220   37   80 4142\n",
            " 1080   23   10 6602 4146   12 1202 8180    2 1100 4558  220   55    4\n",
            "  203 2217    6   37   96   12 2463 2358  621   47   13 3305 2671 3779\n",
            "  688 5308  798    2 1125    8 1210    5    4 4169 2217    6    3 3032\n",
            "  568    3   25 1011   35 3032  568 3033  580 2499   42    3  140 2861\n",
            "  839 3654    6   39 3089   57 4142 1080   44   10 2843 1492    2    8\n",
            "  362  539 2094    6   13    4 1925   75    3 3033  580 2499   42 4292\n",
            " 3317    4 6451  103   13 2377    2   26 4012    2   46  478 1604  238\n",
            " 4754 1264  843   13 3457   97  124 1506 1170   47    8   28  851  584\n",
            " 6256   44    3  861 1125    3 4685 2377   23   26 3731  805    6   51\n",
            "   12 4197 5933 1409 1869   35 2101 1149  498    8  646  920    2 2094\n",
            "  409  429 5706  975   23    4 6451    7 3283 2866   20  124 1043 2568\n",
            "   10    4  474   22 4052    5    4 6335    2 1202 8180    2  179   31\n",
            " 2103 4034  134 3779  688  861 1125    8 3033  580 2499   42 5064  119\n",
            " 4402 2386    2   39 3651  286    4  549 2217    6 4289   20   51    4\n",
            " 2270 1192 5082    6    8   28 1925   75   92 4371   23 1627   74    3\n",
            " 4860   52    3 4230 5222 1459    6    7  987 2866    6    8  741   12\n",
            "  635 4159   23 4478 3792    3    4 1925   75   31 2065   20    3   39\n",
            "   12 3899    2 4558   42    3  165  682 3529    6    7   43 5725 6434\n",
            "    2  339 2576 5811 1205  851   23   35   58 5282 1218    2  646  920\n",
            "    2   10 2843 1492    2 1315    8  780 1192   22 1791 1790    2    3\n",
            "   12 6178  383    7 3869 4901 1455   22 3749 1365   43   39 4402 2386\n",
            "    2    3 1125    4 6451   80  339 1398 1815  815 4089   52 3041  119\n",
            "    8 4571   64  165 3376    6    3 4711 2518  747  407  795 3772    6\n",
            "  118    5    4  724 2921  138 2270  167   10 2843 1492    2    3 1791\n",
            " 1790    2 1125    3   39 1852 1693 2094   32    4 1925   75    4 5624\n",
            "  447    5    4  130    8 4715 6589    2 1693   37 1778  500 6228    4\n",
            " 2270   52    7 1925   75 6082 1717  165  227    3   89 1125    3   71\n",
            "   31 3384    2   35    4  569   13 2063 3150  142   13 1176 5239  533\n",
            "    8  179   36    6 1081   35  569    7 3089   57 2879 1693   13  282\n",
            " 2671    3 1791 1790    2 1125 1202 8180    2 5791 4369   13 1176 3793\n",
            " 2901    2  961  688   68 1228    3  810 4558  220 4142 1080   23   51\n",
            "  871 2706   32 3671   58 2209 1611   10 6602 4146   12   80 4649   42\n",
            "   13    4 2034 2919   75  913 6045  135    3  256  162 3305  896 3614\n",
            "   20    8   28 2180    3  372    5   43 6153   49 5534 2426 1416 5778\n",
            "    3 2245    4 4050  271    5   12 6602 4146   12  695 1203   37 1613\n",
            "    6    4  225    5  345 6451   10 2209 1611    8   28  913 6045  135\n",
            " 1151   43 1897   14 5098    2 6198 3317 2124   35    4 6602 4146   43\n",
            " 4558  220    3   69   55 1048 1126    2  169  850 1496  691    3 5308\n",
            "  798    2 1125 1202 8180    2  755  151   96 3211  344 4952   20  124\n",
            " 5773  688   10    4 5748 5237 3364    8   28 6094   63   14 5098    2\n",
            "  913 6045  135    5 2357  467  650 1042 4686    3 1048 1126    2  169\n",
            " 3904 4587    6    3 3650    6  241 3375  549  851  584 6256   44    3\n",
            "  346 5189    6    3  401 6477    2    7  131 4169 2217    6    8 2473\n",
            "    5    4 4558  220   96   12 6337 2824    5  241 1004   13 1090  227\n",
            "    3 4952    2  124 5384   23 5693 4651  166   10    4 3869   20    8\n",
            " 5920   36    6 4685  523  162   96  185 1602 2688    6   46 5525   26\n",
            "   51  857  124  711    8  378 1639  970    6 5355    2 2064  400 1963\n",
            " 5851  931  329  162 2330   36   47 1005  169 5516    2    8 5920   92\n",
            " 2681   44    4 5525   26    7 2235   32  813 3488   44    8 1073  326\n",
            "  968    2 5473    4 5122 3736   51 2843 1492    2   13 3532 1504    6\n",
            "    3 3429 4244   22    3   26 1620  790  288    7  350   13 2410 1340\n",
            "  402 1049 6140  602  288   21    1    0 2466 3172 5595    2 4558  220\n",
            "   55 4142 1080   23   51 1925   75   58 2270 1192 5082    6   10 2843\n",
            " 1492    2 4030  210 4656 5773  377 2675 5000 4202   10   12  237  531\n",
            " 1690  147    7 5782 1545 3274  369 4030  210  755   80 4649   42   13\n",
            " 3089   57  913 6045  135   10 2034 2919   75 4030  210  755 4694  159\n",
            " 4687    2    4 5267  167    5  131 4169 2217    6 4030    2    1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the article and its summary\n",
        "print('Article:\\n\\n', detokenize(input_batch[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXaDI3ztNk17",
        "outputId": "8c3bcfd4-1445-4d1b-c2e5-ee5e1c4fe21f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article:\n",
            "\n",
            " After 14 years of undernourishment and overcrowding, Elena, Kaitlyn\n",
            "and Alyssa heard the roar of other rescued African lions Wednesday at\n",
            "their spacious new home in Colorado. The siblings arrived earlier in\n",
            "the day at the Wild Animal Sanctuary, about 30 miles northeast of\n",
            "Denver, officials said. The lionesses had spent their entire lives in\n",
            "a fairgrounds zoo in La Chorrera, outside Panama City. The sanctuary\n",
            "helped rescue the lionesses at the behest of the Panamanian government\n",
            "and its National Environmental Authority (ANAM), said Pat Craig,\n",
            "executive director of the sanctuary in Keenesburg. The lionesses, at\n",
            "200 to 225 pounds, weigh about half of what they should. \"Nobody\n",
            "really took care of them,\" Craig said. \"They got no medical\n",
            "attention.\" Non-governmental organizations ran the fairgrounds for\n",
            "years as a way to raise money, Craig said. The lions usually were\n",
            "housed in a 6-foot-by-8-foot concrete and steel cage in a zoo at the\n",
            "fairgrounds. \"These three girls didn't have a family structure,\" Craig\n",
            "said. \"It was more or less that they were isolated for (nearly) 15\n",
            "years.\" Wednesday afternoon, they were in a 1,500-square-foot\n",
            "temporary area at the sanctuary. \"They are in their enclosure, and\n",
            "they are resting comfortably,\" according to Craig. \"They are enjoying\n",
            "the space.\" He expects that the lionesses will eventually join a pride\n",
            "of African lions that were rescued in Bolivia. \"African lions are the\n",
            "only cats that have a strong instinct to live together,\" Craig said.\n",
            "One of the big cats, Elena, is named for Elena Castejon, who actively\n",
            "assists with animal rescues in Panama. On several visits to the zoo,\n",
            "Castejon noticed the animals had to depend on rain or leftover\n",
            "cleaning water to quench their thirst. The carnivores, she said,\n",
            "largely depended on scraps from a slaughterhouse for sustenance. Zoo\n",
            "visitors often bothered the animals and rattled their cages in the wee\n",
            "hours of the night. \"It was horrible,\" she said. Castejon credits ANAM\n",
            "with getting the large cats removed from the fairgrounds. The zoo also\n",
            "featured gnus, deer, crocodiles and turtles. After a protracted legal\n",
            "battle, the zoo was closed, with a mountain lion, two jaguars and an\n",
            "ocelot being temporarily cared for at Summit Zoo in Panama City. Jorge\n",
            "Garcia, a biologist and wildlife technician with ANAM, said the\n",
            "animals were being kept without proper permits. Over two weeks, La\n",
            "Chorrera hosts one of the biggest fairs in Panama, Garcia said, with\n",
            "school groups visiting the zoo the remainder of the year. Because\n",
            "groups that sponsored the fair and zoo changed every two years, he\n",
            "said, it was difficult for the government to force organizers to make\n",
            "improvements. It's important for government and animal rights groups\n",
            "to work together, Garcia said. \"We need to make alliances.\" In\n",
            "February, 25 lions rescued from harrowing conditions at circuses in\n",
            "Bolivia were flown to the Colorado sanctuary, where they live uncaged.\n",
            "The flight, part of an Animal Defenders International operation,\n",
            "followed the passage of a Bolivia 2009 law that bans the use of any\n",
            "animals in circuses. The sanctuary built an 80-acre fenced site for\n",
            "the Bolivian lions, which are split into four prides, Craig said.\n",
            "\"They all have practically doubled their weight\" in the past seven\n",
            "months. The 720-acre sanctuary of rolling grasslands, split into\n",
            "habitats, houses about 300 large carnivores, including bears, tigers\n",
            "and other big cats. Most of the lions have a lifespan of about 21 to\n",
            "23 years, double their expected longevity in the wild. That's largely\n",
            "because they have no predators or competition from within their\n",
            "population. Lionesses receive estrogen implants so they won't go into\n",
            "heat. That also reduces the competition and fighting among males.\n",
            "FedEx carried the felines from Panama to Memphis, Tennessee, on\n",
            "Tuesday and then to Denver early Wednesday.<EOS><pad>Threesister lions\n",
            "are rescued from zoo at fairgrounds in Panama . Underweight siblings\n",
            "lived in a small concrete and metal enclosure . They were flown to\n",
            "animal sanctuary in Colorado . They already can hear the roars of\n",
            "other big cats .<EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tensor(t):\n",
        "    \"\"\"Create tensor from list of lists\"\"\"\n",
        "    return jnp.array(t)\n",
        "\n",
        "\n",
        "def display_tensor(t, name):\n",
        "    \"\"\"Display shape and tensor\"\"\"\n",
        "    print(f'{name} shape: {t.shape}\\n')\n",
        "    print(f'{t}\\n')"
      ],
      "metadata": {
        "id": "qSTl6m1fOD4Y"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
        "display_tensor(q, 'query')\n",
        "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
        "display_tensor(k, 'key')\n",
        "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
        "display_tensor(v, 'value')\n",
        "m = create_tensor([[0, 0], [-1e9, 0]])\n",
        "display_tensor(m, 'mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gya0fjywOZ2K",
        "outputId": "25201767-53f1-4662-d968-286a683ceda8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "key shape: (2, 3)\n",
            "\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "value shape: (2, 3)\n",
            "\n",
            "[[0 1 0]\n",
            " [1 0 1]]\n",
            "\n",
            "mask shape: (2, 2)\n",
            "\n",
            "[[ 0.e+00  0.e+00]\n",
            " [-1.e+09  0.e+00]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
        "display_tensor(q_dot_k, 'query dot key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdKQzo98OjHq",
        "outputId": "9fb9b4a0-75e3-4932-ad27-e098b7657360"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query dot key shape: (2, 2)\n",
            "\n",
            "[[0.57735026 2.309401  ]\n",
            " [1.1547005  2.8867512 ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked = q_dot_k + m\n",
        "display_tensor(masked, 'masked query dot key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcxylWGbOlpU",
        "outputId": "b81d3c03-1ff6-4d3d-974b-21e5040641cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked query dot key shape: (2, 2)\n",
            "\n",
            "[[ 5.7735026e-01  2.3094010e+00]\n",
            " [-1.0000000e+09  2.8867512e+00]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_tensor(masked @ v, 'masked query dot key dot value')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3LxwKuzOzdH",
        "outputId": "a52e3799-cf0e-45af-b306-afb10302df8f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "masked query dot key dot value shape: (2, 3)\n",
            "\n",
            "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
            " [ 2.8867512e+00 -1.0000000e+09  2.8867512e+00]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_with_batch = q[None,:]\n",
        "display_tensor(q_with_batch, 'query with batch dim')\n",
        "k_with_batch = k[None,:]\n",
        "display_tensor(k_with_batch, 'key with batch dim')\n",
        "v_with_batch = v[None,:]\n",
        "display_tensor(v_with_batch, 'value with batch dim')\n",
        "m_bool = create_tensor([[True, True], [False, True]])\n",
        "display_tensor(m_bool, 'boolean mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwKBQDjBO6w8",
        "outputId": "045b3da1-9989-4ab6-b315-1d26e8903840"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n",
            "key with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[1 2 3]\n",
            "  [4 5 6]]]\n",
            "\n",
            "value with batch dim shape: (1, 2, 3)\n",
            "\n",
            "[[[0 1 0]\n",
            "  [1 0 1]]]\n",
            "\n",
            "boolean mask shape: (2, 2)\n",
            "\n",
            "[[ True  True]\n",
            " [False  True]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C1\n",
        "# GRADED FUNCTION: DotProductAttention\n",
        "def DotProductAttention(query, key, value, mask):\n",
        "    \"\"\"Dot product self-attention.\n",
        "    Args:\n",
        "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
        "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
        "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
        "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
        "\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
        "    \"\"\"\n",
        "\n",
        "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
        "    depth = query.shape[-1]\n",
        "\n",
        "    # Calculate scaled query key dot product according to formula above\n",
        "    dots = jnp.matmul(query, jnp.swapaxes(key, 1, 2)) / jnp.sqrt(depth)\n",
        "    \n",
        "    # Apply the mask\n",
        "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
        "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
        "    \n",
        "    # Softmax formula implementation\n",
        "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
        "    # Hint: Last axis should be used and keepdims should be True\n",
        "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
        "    logsumexp = trax.fastmath.logsumexp(dots,axis=-1, keepdims=True)\n",
        "\n",
        "    # Take exponential of dots minus logsumexp to get softmax\n",
        "    # Use jnp.exp()\n",
        "    dots = jnp.exp( dots - logsumexp)\n",
        "\n",
        "    # Multiply dots by value to get self-attention\n",
        "    # Use jnp.matmul()\n",
        "    attention = jnp.matmul(dots, value)\n",
        "\n",
        "    ## END CODE HERE ###\n",
        "    \n",
        "    return attention"
      ],
      "metadata": {
        "id": "L2d5GtvyPCRI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDvTi8NdPJJ5",
        "outputId": "900e7a23-d83c-45cf-ee13-1eff9e2e4b5c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.8496746 , 0.15032546, 0.8496746 ],\n",
              "              [1.        , 0.        , 1.        ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tensor2d = create_tensor(q)\n",
        "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
        "\n",
        "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
        "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
        "\n",
        "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
        "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
        "\n",
        "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
        "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nr7mg4VPYUq",
        "outputId": "908dff24-ade1-4796-f2dc-9f1993842f00"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query matrix (2D tensor) shape: (2, 3)\n",
            "\n",
            "[[1 0 0]\n",
            " [0 1 0]]\n",
            "\n",
            "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
            "\n",
            "[[[[1 0 0]\n",
            "   [0 1 0]]\n",
            "\n",
            "  [[1 0 0]\n",
            "   [0 1 0]]]\n",
            "\n",
            "\n",
            " [[[1 0 0]\n",
            "   [0 1 0]]\n",
            "\n",
            "  [[1 0 0]\n",
            "   [0 1 0]]]]\n",
            "\n",
            "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n",
            "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: compute_attention_heads_closure\n",
        "def compute_attention_heads_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_heads function\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_attention_heads(x):\n",
        "        \"\"\" Compute the attention heads.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        \"\"\"\n",
        "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "        \n",
        "        # Size of the x's batch dimension\n",
        "        batch_size = x.shape[0]\n",
        "        # Length of the sequence\n",
        "        # Should be size of x's first dimension without counting the batch dim\n",
        "        seqlen = x.shape[1]\n",
        "        # Reshape x using jnp.reshape()\n",
        "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
        "        x = jnp.reshape(x, (batch_size, seqlen,n_heads, d_head))\n",
        "        # Transpose x using jnp.transpose()\n",
        "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
        "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "        # Reshape x using jnp.reshape()\n",
        "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
        "        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    return compute_attention_heads"
      ],
      "metadata": {
        "id": "nCE0bySJS4uR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_tensor(tensor3dc3b, \"input tensor\")\n",
        "result_cah = compute_attention_heads_closure(2,3)(tensor3dc3b)\n",
        "display_tensor(result_cah, \"output tensor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lffyZNRiS_oY",
        "outputId": "1cbdb002-67e6-466a-b862-1ab80737202b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n",
            "output tensor shape: (6, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: dot_product_self_attention\n",
        "def dot_product_self_attention(q, k, v):\n",
        "    \"\"\" Masked dot product self attention.\n",
        "    Args:\n",
        "        q (jax.interpreters.xla.DeviceArray): queries.\n",
        "        k (jax.interpreters.xla.DeviceArray): keys.\n",
        "        v (jax.interpreters.xla.DeviceArray): values.\n",
        "    Returns:\n",
        "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
        "    mask_size = q.shape[1]\n",
        "\n",
        "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
        "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
        "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
        "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return DotProductAttention(q, k, v, mask)"
      ],
      "metadata": {
        "id": "BrQJ4adMTRSY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4znJ0e9pTU7Y",
        "outputId": "ed166db2-bcf0-4de6-a6c8-efc4e2b41d7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.        , 1.        , 0.        ],\n",
              "              [0.8496746 , 0.15032548, 0.8496746 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C4\n",
        "# GRADED FUNCTION: compute_attention_output_closure\n",
        "def compute_attention_output_closure(n_heads, d_head):\n",
        "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
        "    Args:\n",
        "        d_head (int):  dimensionality of heads.\n",
        "        n_heads (int): number of attention heads.\n",
        "    Returns:\n",
        "        function: compute_attention_output function\n",
        "    \"\"\"\n",
        "    \n",
        "    def compute_attention_output(x):\n",
        "        \"\"\" Compute the attention output.\n",
        "        Args:\n",
        "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
        "        Returns:\n",
        "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
        "        \"\"\"\n",
        "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "        \n",
        "        # Length of the sequence\n",
        "        # Should be size of x's first dimension without counting the batch dim\n",
        "        seqlen = x.shape[1]\n",
        "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
        "        x = jnp.reshape(x,(-1, n_heads, seqlen, d_head))\n",
        "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
        "        x = jnp.transpose(x, (0,2,1,3)) \n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Reshape to allow to concatenate the heads\n",
        "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
        "    \n",
        "    return compute_attention_output"
      ],
      "metadata": {
        "id": "nnQXdg2BTXtO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_tensor(result_cah, \"input tensor\")\n",
        "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
        "display_tensor(result_cao, \"output tensor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GptCT6nTenV",
        "outputId": "cc4ea42c-dbcf-4f1c-cb65-b2fef558534e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input tensor shape: (6, 2, 3)\n",
            "\n",
            "[[[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]\n",
            "\n",
            " [[1 0 0]\n",
            "  [0 1 0]]]\n",
            "\n",
            "output tensor shape: (3, 2, 6)\n",
            "\n",
            "[[[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]\n",
            "\n",
            " [[1 0 0 1 0 0]\n",
            "  [0 1 0 0 1 0]]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C5\n",
        "# GRADED FUNCTION: CausalAttention\n",
        "def CausalAttention(d_feature, \n",
        "                    n_heads, \n",
        "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
        "                    dot_product_self_attention=dot_product_self_attention,\n",
        "                    compute_attention_output_closure=compute_attention_output_closure,\n",
        "                    mode='train'):\n",
        "    \"\"\"Transformer-style multi-headed causal attention.\n",
        "\n",
        "    Args:\n",
        "        d_feature (int):  dimensionality of feature embedding.\n",
        "        n_heads (int): number of attention heads.\n",
        "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
        "        dot_product_self_attention (function): dot_product_self_attention function. \n",
        "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
        "        mode (str): 'train' or 'eval'.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
        "    \"\"\"\n",
        "    \n",
        "    assert d_feature % n_heads == 0\n",
        "    d_head = d_feature // n_heads\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
        "    # Since you are dealing with closures you might need to call the outer \n",
        "    # function with the correct parameters to get the actual uncalled function.\n",
        "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
        "        \n",
        "\n",
        "    return tl.Serial(\n",
        "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
        "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
        "        ),\n",
        "        \n",
        "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
        "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
        "        # Since you are dealing with closures you might need to call the outer \n",
        "        # function with the correct parameters to get the actual uncalled function.\n",
        "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
        "        tl.Dense(d_feature) # Final dense layer\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###"
      ],
      "metadata": {
        "id": "_L-C_ncpTjLI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the causal attention model\n",
        "print(CausalAttention(d_feature=512, n_heads=8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbYZcnhvTpId",
        "outputId": "eda3232f-f257-4347-8cf3-a1076e130111"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Branch_out3[\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "    [Dense_512, AttnHeads]\n",
            "  ]\n",
            "  DotProductAttn_in3\n",
            "  AttnOutput\n",
            "  Dense_512\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C6\n",
        "# GRADED FUNCTION: DecoderBlock\n",
        "def DecoderBlock(d_model, d_ff, n_heads,\n",
        "                 dropout, mode, ff_activation):\n",
        "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
        "\n",
        "    The input is an activation tensor.\n",
        "\n",
        "    Args:\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        mode (str): 'train' or 'eval'.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # Create masked multi-head attention block using CausalAttention function\n",
        "    causal_attention = CausalAttention( \n",
        "                        d_model,\n",
        "                        n_heads=n_heads,\n",
        "                        mode=mode\n",
        "                        )\n",
        "\n",
        "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
        "    feed_forward = [ \n",
        "        # Normalize layer inputs\n",
        "        tl.LayerNorm(),\n",
        "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_ff),\n",
        "        # Add activation function passed in as a parameter (you need to call it!)\n",
        "        ff_activation(), # Generally ReLU\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode),\n",
        "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
        "        tl.Dense(d_model),\n",
        "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
        "        tl.Dropout(rate=dropout, mode=mode)\n",
        "    ]\n",
        "\n",
        "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
        "    return [\n",
        "      tl.Residual(\n",
        "          # Normalize layer input\n",
        "          tl.LayerNorm(),\n",
        "          # Add causal attention block previously defined (without parentheses)\n",
        "          causal_attention,\n",
        "          # Add dropout with rate and mode specified\n",
        "          tl.Dropout()\n",
        "        ),\n",
        "      tl.Residual(\n",
        "          # Add feed forward block (without parentheses)\n",
        "          feed_forward\n",
        "        ),\n",
        "      ]\n",
        "    ### END CODE HERE ###"
      ],
      "metadata": {
        "id": "-4-81W0ITvZ0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the decoder block\n",
        "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNgEXBNqT38O",
        "outputId": "b8a543b6-fd48-418e-f79d-e5144f83bc27"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Serial[\n",
            "        Branch_out3[\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "          [Dense_512, AttnHeads]\n",
            "        ]\n",
            "        DotProductAttn_in3\n",
            "        AttnOutput\n",
            "        Dense_512\n",
            "      ]\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "], Serial[\n",
            "  Branch_out2[\n",
            "    None\n",
            "    Serial[\n",
            "      LayerNorm\n",
            "      Dense_2048\n",
            "      Serial[\n",
            "        Relu\n",
            "      ]\n",
            "      Dropout\n",
            "      Dense_512\n",
            "      Dropout\n",
            "    ]\n",
            "  ]\n",
            "  Add_in2\n",
            "]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C7\n",
        "# GRADED FUNCTION: TransformerLM\n",
        "def TransformerLM(vocab_size=33300,\n",
        "                  d_model=512,\n",
        "                  d_ff=2048,\n",
        "                  n_layers=6,\n",
        "                  n_heads=8,\n",
        "                  dropout=0.1,\n",
        "                  max_len=4096,\n",
        "                  mode='train',\n",
        "                  ff_activation=tl.Relu):\n",
        "    \"\"\"Returns a Transformer language model.\n",
        "\n",
        "    The input to the model is a tensor of tokens. (This model uses only the\n",
        "    decoder part of the overall Transformer.)\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): vocab size.\n",
        "        d_model (int):  depth of embedding.\n",
        "        d_ff (int): depth of feed-forward layer.\n",
        "        n_layers (int): number of decoder layers.\n",
        "        n_heads (int): number of attention heads.\n",
        "        dropout (float): dropout rate (how much to drop out).\n",
        "        max_len (int): maximum symbol length for positional encoding.\n",
        "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
        "        ff_activation (function): the non-linearity in feed-forward layer.\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
        "        to activations over a vocab set.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # Embedding inputs and positional encoder\n",
        "    positional_encoder = [ \n",
        "        # Add embedding layer of dimension (vocab_size, d_model)\n",
        "        tl.Embedding(vocab_size,d_model),\n",
        "        # Use dropout with rate and mode specified\n",
        "        tl.Dropout(rate = dropout, mode = mode),\n",
        "        # Add positional encoding layer with maximum input length and mode specified\n",
        "        tl.PositionalEncoding()]\n",
        "\n",
        "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
        "    decoder_blocks = [ \n",
        "        DecoderBlock(d_model, d_ff, n_heads,\n",
        "                 dropout, mode, ff_activation) for _ in range(n_layers)]\n",
        "\n",
        "    # Create the complete model as written in the figure\n",
        "    return tl.Serial(\n",
        "        # Use teacher forcing (feed output of previous step to current step)\n",
        "        tl.ShiftRight(), # Specify the mode!\n",
        "        # Add positional encoder\n",
        "        positional_encoder,\n",
        "        # Add decoder blocks\n",
        "        decoder_blocks,\n",
        "        # Normalize layer\n",
        "        tl.LayerNorm(),\n",
        "\n",
        "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
        "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
        "        tl.Dense(vocab_size),\n",
        "        # Get probabilities with Logsoftmax\n",
        "        tl.LogSoftmax()\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###"
      ],
      "metadata": {
        "id": "WmfWhNRVUFbK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the Transformer\n",
        "print(TransformerLM(n_layers=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDq4ZB6lUHjV",
        "outputId": "4cae68c9-f29d-4544-c98a-f5866b798561"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_33300_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Serial[\n",
            "          Branch_out3[\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "            [Dense_512, AttnHeads]\n",
            "          ]\n",
            "          DotProductAttn_in3\n",
            "          AttnOutput\n",
            "          Dense_512\n",
            "        ]\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  Serial[\n",
            "    Branch_out2[\n",
            "      None\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Serial[\n",
            "          Relu\n",
            "        ]\n",
            "        Dropout\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    Add_in2\n",
            "  ]\n",
            "  LayerNorm\n",
            "  Dense_33300\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "# UNQ_C8\n",
        "# GRADED FUNCTION: train_model\n",
        "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
        "    '''\n",
        "    Input:\n",
        "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
        "        train_gen (generator): Training stream of data.\n",
        "        eval_gen (generator): Evaluation stream of data.\n",
        "        output_dir (str): folder to save your file.\n",
        "        \n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop.\n",
        "    '''\n",
        "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
        "\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    train_task = training.TrainTask( \n",
        "      labeled_data=train_gen, # The training generator\n",
        "      loss_layer= tl.CrossEntropyLoss(), # Loss function \n",
        "      optimizer= trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
        "      lr_schedule= lr_schedule,\n",
        "      n_steps_per_checkpoint = 10\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask( \n",
        "      labeled_data=eval_gen, # The evaluation generator\n",
        "      metrics=[tl.CrossEntropyLoss(),tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
        "    )\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    loop = training.Loop(TransformerLM(d_model=4,\n",
        "                                       d_ff=16,\n",
        "                                       n_layers=1,\n",
        "                                       n_heads=2,\n",
        "                                       mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    \n",
        "    return loop"
      ],
      "metadata": {
        "id": "Qj9-ilSiUMCA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Should take around 1.5 minutes\n",
        "!rm -f ~/model/model.pkl.gz\n",
        "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
        "loop.run(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBUiCG1bUTrW",
        "outputId": "712e1169-2404-4edc-f6f0-189fd01cc7fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:413: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 308144\n",
            "Step      1: Ran 1 train steps in 20.07 secs\n",
            "Step      1: train CrossEntropyLoss |  10.41183758\n",
            "Step      1: eval  CrossEntropyLoss |  10.41271687\n",
            "Step      1: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step     10: Ran 9 train steps in 119.93 secs\n",
            "Step     10: train CrossEntropyLoss |  10.41368771\n",
            "Step     10: eval  CrossEntropyLoss |  10.41452026\n",
            "Step     10: eval          Accuracy |  0.00000000\n",
            "\n",
            "Step     20: Ran 10 train steps in 133.80 secs\n",
            "Step     20: train CrossEntropyLoss |  10.41210747\n",
            "Step     20: eval  CrossEntropyLoss |  10.40925312\n",
            "Step     20: eval          Accuracy |  0.00000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model architecture\n",
        "model = TransformerLM(mode='eval')\n",
        "\n",
        "# Load the pre-trained weights\n",
        "model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz', weights_only=True)"
      ],
      "metadata": {
        "id": "f8BHydXWYWDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C9\n",
        "def next_symbol(cur_output_tokens, model):\n",
        "    \"\"\"Returns the next symbol for a given sentence.\n",
        "\n",
        "    Args:\n",
        "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
        "        model (trax.layers.combinators.Serial): The transformer model.\n",
        "\n",
        "    Returns:\n",
        "        int: tokenized symbol.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    \n",
        "    # current output tokens length\n",
        "    token_length = len(cur_output_tokens)\n",
        "    # calculate the minimum power of 2 big enough to store token_length\n",
        "    # HINT: use np.ceil() and np.log2()\n",
        "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
        "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
        "\n",
        "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
        "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
        "\n",
        "    # model expects a tuple containing two padded tensors (with batch)\n",
        "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
        "    # HINT: output has shape (1, padded_length, vocab_size)\n",
        "    # To get log_probs you need to index output with 0 in the first dim\n",
        "    # token_length in the second dim and all of the entries for the last dim.\n",
        "    log_probs = output[0, token_length, :]\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return int(np.argmax(log_probs))"
      ],
      "metadata": {
        "id": "JCI9cqjciVdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it out!\n",
        "#sentence_test_nxt_symbl = \"Hades warned Persephone about the pomegranates but she ate them anyways.\"\n",
        "#detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])\n"
      ],
      "metadata": {
        "id": "cmsKM3lZifQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C10\n",
        "# Decoding functions.\n",
        "def greedy_decode(input_sentence, model):\n",
        "    \"\"\"Greedy decode function.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (string): a sentence or article.\n",
        "        model (trax.layers.combinators.Serial): Transformer model.\n",
        "\n",
        "    Returns:\n",
        "        string: summary of the input.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # Use tokenize()\n",
        "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
        "    generated_output = [] \n",
        "    cur_output = 0 \n",
        "    EOS = 1 \n",
        "    \n",
        "    while cur_output != EOS:\n",
        "        # Get next symbol\n",
        "        cur_output = next_symbol(cur_output_tokens, model)\n",
        "        # Append next symbol to original sentence\n",
        "        cur_output_tokens.append(cur_output)\n",
        "        # Append next symbol to generated sentence\n",
        "        generated_output.append(cur_output)\n",
        "        print(detokenize(generated_output))\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return detokenize(generated_output)\n"
      ],
      "metadata": {
        "id": "DZ3iSi6Rjsmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it out with a whole article!\n",
        "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
        "print(wrapper.fill(article), '\\n')\n",
        "print(greedy_decode(article, model))"
      ],
      "metadata": {
        "id": "U8Z4HjqXIJ5J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}